<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【Datawhale】【CS224W】图机器学习 2 图的特征工程 | Random-Walker in the Knowledge Category</title><meta name="author" content="Ruijie He"><meta name="copyright" content="Ruijie He"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Datawhale开源学习社区 x 同济子豪兄 Stanford课程中文精讲系列笔记">
<meta property="og:type" content="article">
<meta property="og:title" content="【Datawhale】【CS224W】图机器学习 2 图的特征工程">
<meta property="og:url" content="https://hrjtju.github.io/2023/02/15/2023/02/2023-02-15-DatawhaleCS224W-2/index.html">
<meta property="og:site_name" content="Random-Walker in the Knowledge Category">
<meta property="og:description" content="Datawhale开源学习社区 x 同济子豪兄 Stanford课程中文精讲系列笔记">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://hrjtju.github.io/img/header/fuyu.jpg">
<meta property="article:published_time" content="2023-02-14T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-09T17:02:13.426Z">
<meta property="article:author" content="Ruijie He">
<meta property="article:tag" content="GraphML">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hrjtju.github.io/img/header/fuyu.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://hrjtju.github.io/2023/02/15/2023/02/2023-02-15-DatawhaleCS224W-2/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【Datawhale】【CS224W】图机器学习 2 图的特征工程',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-08-10 01:02:13'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/arisu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">71</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">34</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/header/fuyu.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="Random-Walker in the Knowledge Category"><img class="site-icon" src="/img/logo.jpg"/><span class="site-name">Random-Walker in the Knowledge Category</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【Datawhale】【CS224W】图机器学习 2 图的特征工程</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-02-14T16:00:00.000Z" title="Created 2023-02-15 00:00:00">2023-02-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-08-09T17:02:13.426Z" title="Updated 2023-08-10 01:02:13">2023-08-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/posts/">posts</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">4.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>16min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【Datawhale】【CS224W】图机器学习 2 图的特征工程"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><ul>
<li>Reference<ul>
<li><a target="_blank" rel="noopener" href="https://b23.tv/BNJ1uYL">【图的基本表示【斯坦福CS224W图机器学习】-哔哩哔哩</a></li>
<li><a target="_blank" rel="noopener" href="https://people.math.harvard.edu/~knill/teaching/math19b_2011/handouts/lecture34.pdf">Math19b:Linear Algebra with Probability, Lecture 34: Perron Frobenius theorem</a></li>
<li>Shervashidze, Nino, et al. “Efficient graphlet kernels for large graph comparison.” Artificial Intelligence and Statistics. 2009</li>
<li>Shervashidze, Nino, et al. “Weisfeiler-lehman graph kernels.” Journal of Machine Learning Research 12.9 (2011).</li>
</ul>
</li>
</ul>
<h2 id="3-0-特征工程的重要性"><a href="#3-0-特征工程的重要性" class="headerlink" title="3.0 特征工程的重要性"></a>3.0 特征工程的重要性</h2><ul>
<li>设计特征</li>
<li>适当数量的特征可以使得模型表现更好<blockquote>
<p><em>维度灾难 (The curse of dimensionality)</em></p>
</blockquote>
</li>
</ul>
<h2 id="3-1-节点层面的特征工程"><a href="#3-1-节点层面的特征工程" class="headerlink" title="3.1 节点层面的特征工程"></a>3.1 节点层面的特征工程</h2><p><em>问题抽象</em></p>
<ul>
<li>给定无向图 $ G = (V, E)$$</li>
<li>使用算法找到一个映射 $ f: V \rightarrow \mathbb{R}$$<br><em>例子</em></li>
<li>半监督节点分类：欺诈用户预测</li>
</ul>
<p><em>构造节点特征</em></p>
<ul>
<li>节点度数</li>
<li>中心度</li>
<li>积聚系数</li>
<li>子图模式 (Graphlets)</li>
</ul>
<h3 id="3-1-1-节点中心度-Node-Centrality"><a href="#3-1-1-节点中心度-Node-Centrality" class="headerlink" title="3.1.1 节点中心度 (Node Centrality)"></a>3.1.1 节点中心度 (Node Centrality)</h3><h4 id="3-1-1-1-特征向量中心度-Eigenvector-Centrality"><a href="#3-1-1-1-特征向量中心度-Eigenvector-Centrality" class="headerlink" title="3.1.1.1 特征向量中心度 (Eigenvector Centrality)"></a>3.1.1.1 特征向量中心度 (Eigenvector Centrality)</h4><ul>
<li><p>节点$v$的重要程度取决于它邻居的重要程度：</p>
<script type="math/tex; mode=display">
c_{v} = \frac{1}{\lambda} \sum\limits_{u \in N(v)}^{} c_{u}</script><p>其中$ \lambda$ 为邻接矩阵$ A$ 的最大特征值<br>事实上，若考虑图中所有节点特征向量中心度组成的向量$ \boldsymbol c$ ，我们有</p>
<script type="math/tex; mode=display">
\lambda \boldsymbol c = A \boldsymbol c</script><p>显然，这是该图邻接矩阵特征向量的定义</p>
</li>
<li>根据 Perron-Frobenious 定理，我们所选择的这个$ \lambda_{\max}$ 总是一个唯一的正数</li>
</ul>
<blockquote>
<p>Remark.<br><strong>Perron Frobenious Theorem</strong> If all entries of a $ n \times n$  matrix $ A$  are positive (a.k.a. $ A &gt; 0$ ), then it has a unique maximal eigenvalue and its eigenvector has positive entries.<br><u>Proof.</u><br>Consider a closed and bounded set<br>$ X := {|\boldsymbol x| = 1: \boldsymbol x \succeq \boldsymbol 0}$ Notice that the matrix $ A$  defines a map $ \displaystyle T(\boldsymbol v) = \frac{A\boldsymbol v}{|A\boldsymbol v|}$  on $ X$ . Since the entries of $ A$  and $ v$  are all positive, $ T(X)$  in contained in the interior of $ X$ . This map is a <em>contraction</em>, there exists $ 0&lt;k&lt;1$  such that $ d(Tx, Ty) \leqslant kd(x, y)$  where $ d(\cdot, \cdot)$  is the <em>geodesic sphere distance</em>.<br>By <em>Banach’s fixed point theorem</em>, there exists a unique fixed point $ v \in X$  such that $ \boldsymbol v = T(\boldsymbol v)$ . We claim that this fixed point is the eigen vector corresponding to the maximum eigenvalue. For every other eigen vector $ \boldsymbol w$  corresponding to the eigen value $ \mu$  with coordinates $ |w_{j}|$  and length $ L$ , we have</p>
<script type="math/tex; mode=display">
| \mu| |\boldsymbol w_{i}| = |\mu \boldsymbol w_{i}| = \left | \sum\limits_{j}^{} A_{i, j}\boldsymbol w_{j} \right | \leqslant \sum\limits_{j}^{} |A_{i, j}| |\boldsymbol w_{j}| = (A|\boldsymbol w|)_{i}</script><p>which shows that $ |\mu|L \leqslant \lambda L$ . </p>
</blockquote>
<h4 id="3-1-1-2-中间点中心度-Betweeness-Centrality"><a href="#3-1-1-2-中间点中心度-Betweeness-Centrality" class="headerlink" title="3.1.1.2 中间点中心度 (Betweeness Centrality)"></a>3.1.1.2 中间点中心度 (Betweeness Centrality)</h4><ul>
<li><p>节点如果出现在多条最短路径中，那么它很重要：</p>
<script type="math/tex; mode=display">
c_{v} = \sum\limits_{x \ne v \ne t} \frac{\#(\text{shortest paths between $s$ and $t$ containing $v$})}{\# (\text{shortest paths between $s$ and $t$})}</script></li>
</ul>
<h4 id="3-1-1-3-距离中心度-Closeness-Centrality"><a href="#3-1-1-3-距离中心度-Closeness-Centrality" class="headerlink" title="3.1.1.3 距离中心度 (Closeness Centrality)"></a>3.1.1.3 距离中心度 (Closeness Centrality)</h4><ul>
<li><p>A node is important if it has small shortest path lengths to all other nodes</p>
<script type="math/tex; mode=display">
c_{v} = \frac{1}{\sum\limits_{u \ne v} \text{shortest path length between $u$ and $v$}}</script></li>
</ul>
<h3 id="3-1-2-积聚系数-Clustering-Coefficient"><a href="#3-1-2-积聚系数-Clustering-Coefficient" class="headerlink" title="3.1.2 积聚系数 (Clustering Coefficient)"></a>3.1.2 积聚系数 (Clustering Coefficient)</h3><ul>
<li><p>积聚系数描述图上某一结点周围节点的集聚程度：</p>
<script type="math/tex; mode=display">
e_{v} = \frac{\# (\text{edges among neighboring nodes})}{\binom{k_{v}}{2}}</script></li>
</ul>
<ul>
<li>其实就是在数以该节点为一个顶点，邻接点为另两点的不同三角形的个数</li>
</ul>
<h3 id="3-1-3-人工定义的子图模式-Graphlets"><a href="#3-1-3-人工定义的子图模式-Graphlets" class="headerlink" title="3.1.3 人工定义的子图模式 (Graphlets)"></a>3.1.3 人工定义的子图模式 (Graphlets)</h3><ul>
<li>通过3.1.2我们会自然地想到，我们不必仅限于使用三角形来描述节点周围的情况，更可以采用更多样的“小”子图以更好地建模节点周围的结构 (Structure)</li>
</ul>
<p><strong>节点的子图特征向量 (Graph Degree Vector, GDV)</strong></p>
<ul>
<li><p>考虑2-5个节点组成的所有连通图，这些图中的不同节点位置共有73种</p>
</li>
<li><p>如果考虑上述的所有73种情况，我们可以构造图中每个节点的特征向量：</p>
<script type="math/tex; mode=display">
\boldsymbol  x_{v} = [x_{v}^{(1)}, x_{v}^{(2)}, \dots, x_{v}^{(73)}]^{T}</script><p>其中$ x_{v}^{(k)}$ 表示节点$ v$ 上图中模式子图的第$ k$ 种节点出现的次数</p>
</li>
</ul>
<h3 id="3-1-4-总结"><a href="#3-1-4-总结" class="headerlink" title="3.1.4 总结"></a>3.1.4 总结</h3><p>描述节点层面的特征可以分为 <strong>重要性型特征(Importance-based Features)</strong> 和 <strong>结构性特征</strong>。</p>
<p>重要性型特征包括：</p>
<ul>
<li>节点的<em>度数</em>：节点与多少节点通过一条边直接相连</li>
<li>节点的<em>中心度</em>：描述节点在全图中的重要程度。可以通过特征值中心度、中间点中心度和距离中心度等不同计算方式取得。不同计算方式的结果也会不同<br>这些方法有助于我们预测图中富有“影响力”的节点</li>
</ul>
<p>结构性特征获取节点周围的拓扑结构</p>
<ul>
<li>节点度数</li>
<li><em>积聚系数</em>：衡量节点周围的联系紧密程度</li>
<li><em>子图特征向量</em>：较好的对节点邻域拓扑的刻画</li>
</ul>
<h2 id="3-2-连接层面的特征工程"><a href="#3-2-连接层面的特征工程" class="headerlink" title="3.2 连接层面的特征工程"></a>3.2 连接层面的特征工程</h2><p>光有节点的特征是不够的。毕竟节点之间的连接是图的一个独有特征。我再把范畴论拿出来说一下（虽然我也是刚学不久）：范畴论被称为 “数学的数学 (The mathematics of mathematics)”，因为数学系统的不同分支都可以完美的嵌入范畴这一框架。而范畴论中的节点，也就是上一篇思考题中提到的Object在其中是被高度抽象化的。换句话说，<em>范畴论所研究的内容是那些节点之间的关系而不是节点本身</em>，这句话放在这里也很合适。</p>
<ul>
<li>预测连接特征的问题中，我们需要预测两个节点之间连接的特征向量（和节点类似）。例如预测两节点之间是否存在连接的药物共服副作用预测</li>
<li>训练这样的问题的主要方法有两种：<ul>
<li><em>随意抹去一些连接，然后让模型预测被抹去的连接特征</em>。这和NLP中随意抹去语句中的一些词汇然后让模型预测这些词汇十分相似</li>
<li>给定一段时间之内的图$ G[t<em>{0}, t</em>{0}’]$  (这意味着图的连接可能随时间变化)，模型输出一个排好序的列表$ L$ ，其中包含了在下一时间段的图$ G[t<em>{1}, t</em>{1}’]$ 中可能出现的连接<ul>
<li>上述方法的测试过程是，已知下一时间段的节点对应的图中的连接数$ n = |E_{new}|$ ，然后从表$ L$ 中选出前$ n$ 个条目，与$E$进行比较</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="3-2-0-通过连接两端节点的预测"><a href="#3-2-0-通过连接两端节点的预测" class="headerlink" title="3.2.0 通过连接两端节点的预测"></a>3.2.0 通过连接两端节点的预测</h3><ul>
<li>定义函数$ c: V \times V \rightarrow \mathbb{R}，(x, y) \mapsto c(x, y)$$<ul>
<li>例如，$ c(x, y)$ 可以是两个节点共有的邻居个数</li>
</ul>
</li>
<li>按照各节点的函数值进行排序</li>
</ul>
<h3 id="3-2-0’-连接预测鸟瞰"><a href="#3-2-0’-连接预测鸟瞰" class="headerlink" title="3.2.0’ 连接预测鸟瞰"></a>3.2.0’ 连接预测鸟瞰</h3><ul>
<li>基于距离的特征 (Distance-based)</li>
<li>基于局部邻域的特征 (Local neighborhood overlap)</li>
<li>基于全局邻域的特征 (Global neighborhood overlap)</li>
</ul>
<h3 id="3-2-1-基于距离的特征"><a href="#3-2-1-基于距离的特征" class="headerlink" title="3.2.1 基于距离的特征"></a>3.2.1 基于距离的特征</h3><ul>
<li>我们可以将两节点间的距离函数定义为该图上对应节点之间的最短距离。如下图所示：<br>![[attachments/Pasted image 20230216095240.png|600]]</li>
<li>但是距离信息并不能反映两点之间有多少公共的邻接节点。读者可以根据上图中给出的相同距离节点对自行验证</li>
</ul>
<h3 id="3-2-2-基于局部邻域的特征"><a href="#3-2-2-基于局部邻域的特征" class="headerlink" title="3.2.2 基于局部邻域的特征"></a>3.2.2 基于局部邻域的特征</h3><h4 id="3-2-2-1-共同邻接点数"><a href="#3-2-2-1-共同邻接点数" class="headerlink" title="3.2.2.1 共同邻接点数"></a>3.2.2.1 共同邻接点数</h4><ul>
<li><p>图$ G = (V, E)$ 中任意两节点之间的公共邻接点数定义为</p>
<script type="math/tex; mode=display">
|N(v_{1}) \cap N(v_{2})|,</script><p>其中$ |\cdot|$ 代表有限集中的元素个数</p>
</li>
</ul>
<h4 id="3-2-2-2-Jaccard-系数"><a href="#3-2-2-2-Jaccard-系数" class="headerlink" title="3.2.2.2 Jaccard 系数"></a>3.2.2.2 Jaccard 系数</h4><ul>
<li><p>Jaccard 系数又称两节点邻接点集的<em>交并比</em></p>
<script type="math/tex; mode=display">
J(v_{1}, v_{2}) = \frac{|N(v_{1})\cap N(v_{2})|}{|N(v_{1})\cup N(v_{2})|}</script></li>
<li><p>一个比较直觉的解释是：两人的所有认识的人中，二人都认识的人所占的比例</p>
</li>
</ul>
<h4 id="3-2-2-3-Adamic-Adar-指数"><a href="#3-2-2-3-Adamic-Adar-指数" class="headerlink" title="3.2.2.3 Adamic-Adar 指数"></a>3.2.2.3 Adamic-Adar 指数</h4><ul>
<li>这个指标由公共邻接点的度数定义</li>
</ul>
<script type="math/tex; mode=display">
  A(v_{1}, v_{2}) = \sum\limits_{u\in N(v_{1})\cap N(v_{2})} \frac{1}{\log (k_{u})}</script><p>  其中$ k_{u}$ 表示节点$ u$ 的度数</p>
<ul>
<li>读者可能发现这个指数和其他有所不同。按照子豪兄的话来讲就是$ v<em>{1}$ 和$ v</em>{2}$ 的共同好友是不是“社牛”，因为“社牛”广交朋友，这可能体现不出两个节点之间的连接的独特特征。</li>
</ul>
<h3 id="3-2-3-基于全局邻域的特征"><a href="#3-2-3-基于全局邻域的特征" class="headerlink" title="3.2.3 基于全局邻域的特征"></a>3.2.3 基于全局邻域的特征</h3><ul>
<li>基于邻接点的特征很有可能出现一个弊端：如果两个结点相距太远，它们的邻接点集合的交集就会变成空集，相应的特征变为零。这样的特征为零的连接是很多的，会为我们的分析带来不小的困难。于是人们发明了基于全局邻域的特征，来解决这个问题</li>
</ul>
<h4 id="3-2-3-1-Katz-指数"><a href="#3-2-3-1-Katz-指数" class="headerlink" title="3.2.3.1 Katz 指数"></a>3.2.3.1 Katz 指数</h4><p><strong>邻接矩阵的幂</strong><br>为简便起见，我们将图$ G$ 的邻接矩阵$ A<em>{G}$ 的$ k$ 次幂记为$ P^{(k)}$ . 可以验证，$ P^{(k)}$ 中元素的几何意义为图中节点$ u$ 和$ v$ 之间$ k$ 跳路径 (path) 的数目，不妨将$ u$ 写作$ n</em>{0}$ ，将$ v$ 写作$ n_{k+1}$$</p>
<script type="math/tex; mode=display">
P^{(k)}_{u, v} = \sum\limits_{n_{0}\ne n_{1} \ne \cdots n_{k+1}} \prod\limits_{i=0}^{k} A_{n_{i}, n_{i+1}} = A^{k}_{u, v}.</script><p><strong>Katz 指数</strong></p>
<ul>
<li>Katz指数衡量两节点之间所有不同长度的$ k$ 跳路径数目的加权和</li>
<li>为了使得总和有限，需要引入衰减因子$ \beta$$</li>
</ul>
<script type="math/tex; mode=display">
S_{v_{1}, v_{2}} := \sum\limits_{l=1}^{\infty} \beta^{l} P^{(l)}_{v_{1}, v_{2}}</script><p>根据几何级数的求和公式，Katz指数矩阵$ \boldsymbol S$ 可以获得闭式解</p>
<script type="math/tex; mode=display">
\boldsymbol S = (\boldsymbol I - \beta \boldsymbol A)^{-1} - \boldsymbol  I</script><blockquote>
<p>虽然Katz指数可以根据闭式解求得，但矩阵逆的计算的消耗是很高的。</p>
</blockquote>
<h3 id="3-2-4-总结"><a href="#3-2-4-总结" class="headerlink" title="3.2.4 总结"></a>3.2.4 总结</h3><ul>
<li><em>基于距离</em>的特征：使用节点的距离作为特征</li>
<li><em>基于局部邻域</em>的特征：使用节点的邻域交集作为特征</li>
<li><em>基于全局邻域</em>的特征：局部邻域特征的延拓，解决了前者的一些问题</li>
</ul>
<h2 id="3-3-全图层面的特征工程"><a href="#3-3-全图层面的特征工程" class="headerlink" title="3.3 全图层面的特征工程"></a>3.3 全图层面的特征工程</h2><h3 id="3-3-0-核方法-Kernel-Method"><a href="#3-3-0-核方法-Kernel-Method" class="headerlink" title="3.3.0 核方法 (Kernel Method)"></a>3.3.0 核方法 (Kernel Method)</h3><ul>
<li>核方法的中心思想是设计一个<em>核</em>来代替特征向量</li>
</ul>
<h4 id="3-3-0-1-核-Kernel"><a href="#3-3-0-1-核-Kernel" class="headerlink" title="3.3.0.1 核 (Kernel)"></a>3.3.0.1 核 (Kernel)</h4><ul>
<li><p>一个核$ K(G, G’) \in \mathbb{R}$ 是数据间相似度的一种度量 (measure)</p>
</li>
<li><p>核矩阵$ \boldsymbol K := \left( K(G, G’) \right)_{G, G’}$  一定总是半正定 (positive semidefinite) 的</p>
</li>
<li><p>存在某个特征映射$ \phi(\cdot)$ 使得</p>
<script type="math/tex; mode=display">
K(G, G') = \phi(G)^{T}\phi(G')</script></li>
<li><p>定义好核以后，就可以将特征交给其他传统机器学习模型了</p>
</li>
</ul>
<h4 id="3-3-0-2-Graph-Kernel"><a href="#3-3-0-2-Graph-Kernel" class="headerlink" title="3.3.0.2 Graph Kernel"></a>3.3.0.2 Graph Kernel</h4><blockquote>
<p>本来想译作”图的核”,但是感觉这样子太奇怪了所以干脆不翻译</p>
</blockquote>
<ul>
<li>Graph Kernel和一般的核一样，可以度量两个图之间的相似度<ul>
<li>Graphlet Kernel</li>
<li>Weisfeiler-Lehman 核</li>
<li>随机游走核 (Random-walk Kernel)</li>
<li>最短路核 (Shortest-path Graph Kernel)</li>
<li>…</li>
</ul>
</li>
</ul>
<p><strong>Bag of words</strong></p>
<ul>
<li>仅仅考虑每种节点的数目</li>
</ul>
<p><strong>Bag of node degrees</strong></p>
<ul>
<li>考虑不同度数的节点数目</li>
</ul>
<h3 id="3-3-1-全图的模式子图特征-Graphlet-Kernel"><a href="#3-3-1-全图的模式子图特征-Graphlet-Kernel" class="headerlink" title="3.3.1 全图的模式子图特征: Graphlet Kernel"></a>3.3.1 全图的模式子图特征: Graphlet Kernel</h3><ul>
<li><p>将图中的模式子图进行计数，得到全图特征向量</p>
</li>
<li><p>考虑不超过$ k$ 各节点的模式子图构造的$ n_{k}$ 元组</p>
<script type="math/tex; mode=display">
\mathcal{G}_{k} := (g_{1}, g_{2}, \dots, g_{n_{k}})</script><p>其中$ n_{k}$ 代表直到$ k$ 个节点的所有子图模式的个数。</p>
</li>
<li><p>将这个$ n<em>{k}$ 元组改写为$ n</em>{k}$ 维向量，就得到了该图的模式子图特征向量</p>
</li>
<li><p>自然地，$ K(G, G’)$ 被定义为模式子图向量之间的内积:</p>
<script type="math/tex; mode=display">
K(G, G') := \boldsymbol f_{G}^{T}\boldsymbol  f_{G'}.</script></li>
<li><p>对于不同大小的图带来的特征向量模长的问题，我们可以将其进行归一化操作:</p>
<script type="math/tex; mode=display">
\boldsymbol  h_{G} = \frac{\boldsymbol f_{G}}{\boldsymbol 1^{T} \boldsymbol f_{G}}</script><p>类似地，$ K(G, G’)$ 也就被定义为归一化向量之间的内积</p>
</li>
</ul>
<p><strong>Graphlet Kernel 的局限</strong></p>
<ul>
<li>当最大模式子图节点数$ k$ 增加时，$ n_{k}$ 增长迅速，这带来了更大的计算量</li>
<li>判断子图之间是否同构的问题极其困难</li>
<li>即使限制了每个节点的度数，以上算法的复杂度依然高至$ O(nd^{k-1})$$</li>
</ul>
<h3 id="3-3-2-Weisfeiler-Lehman-核"><a href="#3-3-2-Weisfeiler-Lehman-核" class="headerlink" title="3.3.2 Weisfeiler-Lehman 核"></a>3.3.2 Weisfeiler-Lehman 核</h3><ul>
<li>为了解决按照子图计数的计算难题，该方法借助节点的邻域结构对节点进行迭代计算</li>
</ul>
<h4 id="3-3-2-1-颜色微调-Color-Refinement"><a href="#3-3-2-1-颜色微调-Color-Refinement" class="headerlink" title="3.3.2.1 颜色微调 (Color Refinement)"></a>3.3.2.1 颜色微调 (Color Refinement)</h4><ul>
<li>首先将所有节点赋值为同一颜色$ c^{(0)}(v)$$</li>
<li>根据每个节点邻域节点的颜色，将这些颜色汇聚于该节点</li>
<li><p>采用哈希表将新的颜色映射为新的数字</p>
<script type="math/tex; mode=display">
c^{(k+1)}(v) = \text{HASH}\left(\left\{ c^{(k)}(v), \left\{ c^{(k)}(u) \right\}_{u \in N(v)} \right\} \right)</script></li>
<li><p>重复执行上述内容若$ k$ 轮，刻画图中$ k$ 跳连接的结构</p>
</li>
<li>最后按照哈希表的顺序将每个图中不同颜色的节点个数生成一个特征向量</li>
<li>算法复杂度为$ O(|E|)$$</li>
</ul>
<h3 id="3-3-3-总结"><a href="#3-3-3-总结" class="headerlink" title="3.3.3 总结"></a>3.3.3 总结</h3><ul>
<li>子图特征：计算消耗大</li>
<li>Weisfeiler-Lehman 核：计算简便</li>
</ul>
<h2 id="3-4-思考题"><a href="#3-4-思考题" class="headerlink" title="3.4 思考题"></a>3.4 思考题</h2><h3 id="3-4-1-全图层面的数据挖掘"><a href="#3-4-1-全图层面的数据挖掘" class="headerlink" title="3.4.1 全图层面的数据挖掘"></a>3.4.1 全图层面的数据挖掘</h3><h4 id="3-4-1-1-全图层面存在哪些数据挖掘任务，有什么应用场景？"><a href="#3-4-1-1-全图层面存在哪些数据挖掘任务，有什么应用场景？" class="headerlink" title="3.4.1.1 全图层面存在哪些数据挖掘任务，有什么应用场景？"></a>3.4.1.1 全图层面存在哪些数据挖掘任务，有什么应用场景？</h4><ul>
<li>判断全图的种类，如预测有效抗生素的结构等</li>
</ul>
<h4 id="3-4-1-2-全图层面可以构造哪些特征？"><a href="#3-4-1-2-全图层面可以构造哪些特征？" class="headerlink" title="3.4.1.2 全图层面可以构造哪些特征？"></a>3.4.1.2 全图层面可以构造哪些特征？</h4><ul>
<li>可以基于邻接矩阵、子图构造对应的特征向量</li>
<li>也可以使用邻接矩阵。因为邻接矩阵核图是一一对应的（这当然仅限于非异质图）</li>
<li>进一步地，我们可以使用神经网络将邻接矩阵投影至特征空间，然后在特征空间上进行其他的操作</li>
</ul>
<h3 id="3-4-2-全图层面的子图匹配"><a href="#3-4-2-全图层面的子图匹配" class="headerlink" title="3.4.2 全图层面的子图匹配"></a>3.4.2 全图层面的子图匹配</h3><h4 id="3-4-2-1-全图层面的-Graphlet-和节点层面的-Graphlet-有何区别？"><a href="#3-4-2-1-全图层面的-Graphlet-和节点层面的-Graphlet-有何区别？" class="headerlink" title="3.4.2.1 全图层面的 Graphlet 和节点层面的 Graphlet 有何区别？"></a>3.4.2.1 全图层面的 Graphlet 和节点层面的 Graphlet 有何区别？</h4><ul>
<li>全图层面的子图结构关心某一个子图结构是否是这个图的一个子图</li>
<li>节点层面的子图结构采用的子图是连通图，并且关心节点在同一子图的不同位置</li>
</ul>
<h4 id="3-4-2-2-子图匹配的复杂度如何计算？"><a href="#3-4-2-2-子图匹配的复杂度如何计算？" class="headerlink" title="3.4.2.2 子图匹配的复杂度如何计算？"></a>3.4.2.2 子图匹配的复杂度如何计算？</h4><ul>
<li>假设所采用的子图节点数为$ k$ ，对图中节点度数进行适当的限制，使其不超过$ d$ 。已知一个子图，想要在全图中搜索该种结构，需要的步数为$ O(nd^{k-1})$$</li>
<li>对于固定的$ k$ ，$ n_{k}$ 即使很大也依然为常数。于是子图匹配的复杂度为$ O(nd^{k-1})$ .</li>
</ul>
<h3 id="3-4-3-Weisfeiler-Lehman-核"><a href="#3-4-3-Weisfeiler-Lehman-核" class="headerlink" title="3.4.3 Weisfeiler-Lehman 核"></a>3.4.3 Weisfeiler-Lehman 核</h3><h4 id="3-4-3-1-简述-Weisfeiler-Lehman-算法的原理"><a href="#3-4-3-1-简述-Weisfeiler-Lehman-算法的原理" class="headerlink" title="3.4.3.1 简述 Weisfeiler-Lehman 算法的原理"></a>3.4.3.1 简述 Weisfeiler-Lehman 算法的原理</h4><ul>
<li>算法采用将邻接点的迭代地特征汇聚到节点的方法来表征图中每个节点周围的拓扑结构</li>
<li>随着迭代次数增加，每个节点的“感受区域”逐渐增加，取适当的迭代次数，可以很好地反映全图的特征</li>
</ul>
<h4 id="3-4-3-2-Weisfeiler-Lehman-核的词汇表是如何构建的？"><a href="#3-4-3-2-Weisfeiler-Lehman-核的词汇表是如何构建的？" class="headerlink" title="3.4.3.2 Weisfeiler-Lehman 核的词汇表是如何构建的？"></a>3.4.3.2 Weisfeiler-Lehman 核的词汇表是如何构建的？</h4><ul>
<li>首先将所有节点赋予相同的初值</li>
<li>在一次迭代过程中，每个节点的词汇被更新为现在在词汇和其邻接点的所有词汇相连的结果</li>
<li>然后根据事先定义的哈希表将每个结点的词汇二元组映射成新的词汇</li>
</ul>
<h4 id="3-4-3-3-Weisfeiler-Lehman-核和图神经网络有何关系？"><a href="#3-4-3-3-Weisfeiler-Lehman-核和图神经网络有何关系？" class="headerlink" title="3.4.3.3 Weisfeiler-Lehman 核和图神经网络有何关系？"></a>3.4.3.3 Weisfeiler-Lehman 核和图神经网络有何关系？</h4><ul>
<li>该算法过程中的迭代过程与GCN中的汇聚运算十分相似</li>
</ul>
<h3 id="3-4-4-核方法-Kernel-Methods"><a href="#3-4-4-核方法-Kernel-Methods" class="headerlink" title="3.4.4 核方法 (Kernel Methods)"></a>3.4.4 核方法 (Kernel Methods)</h3><h4 id="3-4-4-1-简述核方法的基本原理"><a href="#3-4-4-1-简述核方法的基本原理" class="headerlink" title="3.4.4.1 简述核方法的基本原理"></a>3.4.4.1 简述核方法的基本原理</h4><ul>
<li>对于一些难以建模或难以分离的实体，我们采用一种映射，将不同的实体映射为高维空间上的点</li>
<li>这些特征点在高维空间上可以进行内积运算，以获取实体间的某种“度量”</li>
<li>通过这种度量结合传统机器学习任务可以帮助我们解决较为棘手的问题</li>
</ul>
<h4 id="3-4-4-2-为什么在全图层面的任务中使用核方法？"><a href="#3-4-4-2-为什么在全图层面的任务中使用核方法？" class="headerlink" title="3.4.4.2 为什么在全图层面的任务中使用核方法？"></a>3.4.4.2 为什么在全图层面的任务中使用核方法？</h4><ul>
<li>图作为一种不同于常量和矩阵的结构，难以对其进行直接的建模。因此我们可以通过核函数不同的图映射到相同的高维空间，然后再其上进行操作</li>
</ul>
<h4 id="3-4-4-3-除了上述提到的两种核之外，还有哪些核？"><a href="#3-4-4-3-除了上述提到的两种核之外，还有哪些核？" class="headerlink" title="3.4.4.3 除了上述提到的两种核之外，还有哪些核？"></a>3.4.4.3 除了上述提到的两种核之外，还有哪些核？</h4><ul>
<li>如前文中所述，还有最短路核、随机游走核等等</li>
<li>其中随机游走核和图神经网络应该也有一些联系。在一些较大的图中，采用一些通常的操作会造成较大的开销。而采用随机游走的方式可以显著降低计算开销</li>
</ul>
<h3 id="3-4-5-邻接矩阵"><a href="#3-4-5-邻接矩阵" class="headerlink" title="3.4.5 邻接矩阵"></a>3.4.5 邻接矩阵</h3><h4 id="3-4-5-1-传统机器学习核特征工程中，哪些用到了邻接矩阵？"><a href="#3-4-5-1-传统机器学习核特征工程中，哪些用到了邻接矩阵？" class="headerlink" title="3.4.5.1 传统机器学习核特征工程中，哪些用到了邻接矩阵？"></a>3.4.5.1 传统机器学习核特征工程中，哪些用到了邻接矩阵？</h4><ul>
<li>计算节点度数</li>
<li>计算(加权)最短路</li>
<li>计算$n$跳路径个数</li>
<li>…</li>
</ul>
<h4 id="3-4-5-2-如何将把无向图的节点、连接、全图的特征推广到有向图？"><a href="#3-4-5-2-如何将把无向图的节点、连接、全图的特征推广到有向图？" class="headerlink" title="3.4.5.2 如何将把无向图的节点、连接、全图的特征推广到有向图？"></a>3.4.5.2 如何将把无向图的节点、连接、全图的特征推广到有向图？</h4><p><strong>节点特征</strong></p>
<ul>
<li>与无向图的节点相似，只需要注意此时的边是有向的即可</li>
</ul>
<p><strong>连接特征</strong></p>
<ul>
<li>基于邻域的特征可以与无向图保持不变，也可以更改一些节点邻域的定义（比如连接从节点发出变为连接指向节点）以获取不同的特征</li>
</ul>
<p><strong>全图特征</strong></p>
<ul>
<li>WL核的方法在有向图中依然适用。我猜测这种结构会更加强化对于有向图中SCC结构的体现</li>
</ul>
<h3 id="3-4-6-如何使用代码实现-Weisfeiler-Lehman-核？"><a href="#3-4-6-如何使用代码实现-Weisfeiler-Lehman-核？" class="headerlink" title="3.4.6 如何使用代码实现 Weisfeiler-Lehman 核？"></a>3.4.6 如何使用代码实现 Weisfeiler-Lehman 核？</h3><p>在此写出其伪代码：</p>
<script type="math/tex; mode=display">
\begin{align*ed}

&\rule{110mm}{0.4pt} \\

&\textbf{initialize} : \text{Adj matrix $A$, Initial color $c_{0}$，Hash function $H(\cdot)$}
\\[-1.ex]
&\text{Iteration num $N$，Maximum  length $K$ of the output vector}
\\[-1.ex]
&\rule{110mm}{0.4pt} \\
&\textbf{for} \: m=1 \: \textbf{to} \: M \: \textbf{do} \\
&\qquad \text{Create a temporate list } L'\\
&\qquad \textbf{for} \text{ node}\textbf{ in }\text{the index of }L \textbf{ do}\\
&\qquad\qquad L'[node] \leftarrow H(\text{concatenate}_{j}(A_{node, j} L[j]))\\
&\qquad L \leftarrow L' \\
&\textbf{return } L[:K]\\
&\rule{110mm}{0.4pt} \\[-1.ex]
\end{align*ed}</script><h2 id="3-5-疑问"><a href="#3-5-疑问" class="headerlink" title="3.5 疑问"></a>3.5 疑问</h2><h3 id="3-5-1-Katz-Index"><a href="#3-5-1-Katz-Index" class="headerlink" title="3.5.1 Katz Index"></a>3.5.1 Katz Index</h3><p>在Katz Index中，我们采用矩阵幂或者矩阵求逆的运算求解$ K$ ，但矩阵求逆运算的开销是较大的。在实际过程中我们是如何计算Katz Index中的？（特指很大的图结构）</p>
<ul>
<li>限制幂级数的阶，用部分和估计Katz Index</li>
<li>采用某种特殊方法估计$ K$$</li>
</ul>
<h3 id="3-5-2-Weisfeiler-Kernel"><a href="#3-5-2-Weisfeiler-Kernel" class="headerlink" title="3.5.2 Weisfeiler Kernel"></a>3.5.2 Weisfeiler Kernel</h3><p>在Weisfeiler Kernel算法的迭代过程中，迭代次数较大时，是否会出现像GCN中算法一样的“同质化”问题？对于某一张给定的图，是否存在一个较好的，甚至是最佳的迭代次数？</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://hrjtju.github.io">Ruijie He</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://hrjtju.github.io/2023/02/15/2023/02/2023-02-15-DatawhaleCS224W-2/">https://hrjtju.github.io/2023/02/15/2023/02/2023-02-15-DatawhaleCS224W-2/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/GraphML/">GraphML</a></div><div class="post_share"><div class="social-share" data-image="/img/header/fuyu.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/07/17/2023/08/2023-08-10-DRL-7-Value-Functions/" title="【UCBerkley DRL】 7 Value Functions"><img class="cover" src="/img/2023/headers/drl.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">【UCBerkley DRL】 7 Value Functions</div></div></a></div><div class="next-post pull-right"><a href="/2023/02/15/2023/02/2023-02-15-Optim-1/" title="【Convex Optimization】1 Introduction to Convex Optimization and Algorithms"><img class="cover" src="/img/header/optim.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">【Convex Optimization】1 Introduction to Convex Optimization and Algorithms</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2023/02/13/2023/02/2023-02-13-DatawhaleCS224w-1/" title="【Datawhale】【CS224W】图机器学习 1 图的基本表示"><img class="cover" src="/img/header/fuyu.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-13</div><div class="title">【Datawhale】【CS224W】图机器学习 1 图的基本表示</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/arisu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Ruijie He</div><div class="author-info__description">Undergraduate in Tongji University</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">71</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">34</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/hrjtju"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">7月17日给博客换了一个主题，现在仍在装修中</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-0-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E7%9A%84%E9%87%8D%E8%A6%81%E6%80%A7"><span class="toc-text">3.0 特征工程的重要性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E8%8A%82%E7%82%B9%E5%B1%82%E9%9D%A2%E7%9A%84%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-text">3.1 节点层面的特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-1-%E8%8A%82%E7%82%B9%E4%B8%AD%E5%BF%83%E5%BA%A6-Node-Centrality"><span class="toc-text">3.1.1 节点中心度 (Node Centrality)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-1-%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F%E4%B8%AD%E5%BF%83%E5%BA%A6-Eigenvector-Centrality"><span class="toc-text">3.1.1.1 特征向量中心度 (Eigenvector Centrality)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-2-%E4%B8%AD%E9%97%B4%E7%82%B9%E4%B8%AD%E5%BF%83%E5%BA%A6-Betweeness-Centrality"><span class="toc-text">3.1.1.2 中间点中心度 (Betweeness Centrality)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-1-3-%E8%B7%9D%E7%A6%BB%E4%B8%AD%E5%BF%83%E5%BA%A6-Closeness-Centrality"><span class="toc-text">3.1.1.3 距离中心度 (Closeness Centrality)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-2-%E7%A7%AF%E8%81%9A%E7%B3%BB%E6%95%B0-Clustering-Coefficient"><span class="toc-text">3.1.2 积聚系数 (Clustering Coefficient)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-3-%E4%BA%BA%E5%B7%A5%E5%AE%9A%E4%B9%89%E7%9A%84%E5%AD%90%E5%9B%BE%E6%A8%A1%E5%BC%8F-Graphlets"><span class="toc-text">3.1.3 人工定义的子图模式 (Graphlets)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-4-%E6%80%BB%E7%BB%93"><span class="toc-text">3.1.4 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E8%BF%9E%E6%8E%A5%E5%B1%82%E9%9D%A2%E7%9A%84%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-text">3.2 连接层面的特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-0-%E9%80%9A%E8%BF%87%E8%BF%9E%E6%8E%A5%E4%B8%A4%E7%AB%AF%E8%8A%82%E7%82%B9%E7%9A%84%E9%A2%84%E6%B5%8B"><span class="toc-text">3.2.0 通过连接两端节点的预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-0%E2%80%99-%E8%BF%9E%E6%8E%A5%E9%A2%84%E6%B5%8B%E9%B8%9F%E7%9E%B0"><span class="toc-text">3.2.0’ 连接预测鸟瞰</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-1-%E5%9F%BA%E4%BA%8E%E8%B7%9D%E7%A6%BB%E7%9A%84%E7%89%B9%E5%BE%81"><span class="toc-text">3.2.1 基于距离的特征</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-2-%E5%9F%BA%E4%BA%8E%E5%B1%80%E9%83%A8%E9%82%BB%E5%9F%9F%E7%9A%84%E7%89%B9%E5%BE%81"><span class="toc-text">3.2.2 基于局部邻域的特征</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-1-%E5%85%B1%E5%90%8C%E9%82%BB%E6%8E%A5%E7%82%B9%E6%95%B0"><span class="toc-text">3.2.2.1 共同邻接点数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-2-Jaccard-%E7%B3%BB%E6%95%B0"><span class="toc-text">3.2.2.2 Jaccard 系数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-3-Adamic-Adar-%E6%8C%87%E6%95%B0"><span class="toc-text">3.2.2.3 Adamic-Adar 指数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-3-%E5%9F%BA%E4%BA%8E%E5%85%A8%E5%B1%80%E9%82%BB%E5%9F%9F%E7%9A%84%E7%89%B9%E5%BE%81"><span class="toc-text">3.2.3 基于全局邻域的特征</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-1-Katz-%E6%8C%87%E6%95%B0"><span class="toc-text">3.2.3.1 Katz 指数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-4-%E6%80%BB%E7%BB%93"><span class="toc-text">3.2.4 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E5%85%A8%E5%9B%BE%E5%B1%82%E9%9D%A2%E7%9A%84%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B"><span class="toc-text">3.3 全图层面的特征工程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-0-%E6%A0%B8%E6%96%B9%E6%B3%95-Kernel-Method"><span class="toc-text">3.3.0 核方法 (Kernel Method)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-0-1-%E6%A0%B8-Kernel"><span class="toc-text">3.3.0.1 核 (Kernel)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-0-2-Graph-Kernel"><span class="toc-text">3.3.0.2 Graph Kernel</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-1-%E5%85%A8%E5%9B%BE%E7%9A%84%E6%A8%A1%E5%BC%8F%E5%AD%90%E5%9B%BE%E7%89%B9%E5%BE%81-Graphlet-Kernel"><span class="toc-text">3.3.1 全图的模式子图特征: Graphlet Kernel</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-2-Weisfeiler-Lehman-%E6%A0%B8"><span class="toc-text">3.3.2 Weisfeiler-Lehman 核</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-2-1-%E9%A2%9C%E8%89%B2%E5%BE%AE%E8%B0%83-Color-Refinement"><span class="toc-text">3.3.2.1 颜色微调 (Color Refinement)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-3-%E6%80%BB%E7%BB%93"><span class="toc-text">3.3.3 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E6%80%9D%E8%80%83%E9%A2%98"><span class="toc-text">3.4 思考题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-1-%E5%85%A8%E5%9B%BE%E5%B1%82%E9%9D%A2%E7%9A%84%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98"><span class="toc-text">3.4.1 全图层面的数据挖掘</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-1-1-%E5%85%A8%E5%9B%BE%E5%B1%82%E9%9D%A2%E5%AD%98%E5%9C%A8%E5%93%AA%E4%BA%9B%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E4%BB%BB%E5%8A%A1%EF%BC%8C%E6%9C%89%E4%BB%80%E4%B9%88%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%EF%BC%9F"><span class="toc-text">3.4.1.1 全图层面存在哪些数据挖掘任务，有什么应用场景？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-1-2-%E5%85%A8%E5%9B%BE%E5%B1%82%E9%9D%A2%E5%8F%AF%E4%BB%A5%E6%9E%84%E9%80%A0%E5%93%AA%E4%BA%9B%E7%89%B9%E5%BE%81%EF%BC%9F"><span class="toc-text">3.4.1.2 全图层面可以构造哪些特征？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-2-%E5%85%A8%E5%9B%BE%E5%B1%82%E9%9D%A2%E7%9A%84%E5%AD%90%E5%9B%BE%E5%8C%B9%E9%85%8D"><span class="toc-text">3.4.2 全图层面的子图匹配</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-2-1-%E5%85%A8%E5%9B%BE%E5%B1%82%E9%9D%A2%E7%9A%84-Graphlet-%E5%92%8C%E8%8A%82%E7%82%B9%E5%B1%82%E9%9D%A2%E7%9A%84-Graphlet-%E6%9C%89%E4%BD%95%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-text">3.4.2.1 全图层面的 Graphlet 和节点层面的 Graphlet 有何区别？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-2-2-%E5%AD%90%E5%9B%BE%E5%8C%B9%E9%85%8D%E7%9A%84%E5%A4%8D%E6%9D%82%E5%BA%A6%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%EF%BC%9F"><span class="toc-text">3.4.2.2 子图匹配的复杂度如何计算？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-3-Weisfeiler-Lehman-%E6%A0%B8"><span class="toc-text">3.4.3 Weisfeiler-Lehman 核</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-3-1-%E7%AE%80%E8%BF%B0-Weisfeiler-Lehman-%E7%AE%97%E6%B3%95%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-text">3.4.3.1 简述 Weisfeiler-Lehman 算法的原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-3-2-Weisfeiler-Lehman-%E6%A0%B8%E7%9A%84%E8%AF%8D%E6%B1%87%E8%A1%A8%E6%98%AF%E5%A6%82%E4%BD%95%E6%9E%84%E5%BB%BA%E7%9A%84%EF%BC%9F"><span class="toc-text">3.4.3.2 Weisfeiler-Lehman 核的词汇表是如何构建的？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-3-3-Weisfeiler-Lehman-%E6%A0%B8%E5%92%8C%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%9C%89%E4%BD%95%E5%85%B3%E7%B3%BB%EF%BC%9F"><span class="toc-text">3.4.3.3 Weisfeiler-Lehman 核和图神经网络有何关系？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-4-%E6%A0%B8%E6%96%B9%E6%B3%95-Kernel-Methods"><span class="toc-text">3.4.4 核方法 (Kernel Methods)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-4-1-%E7%AE%80%E8%BF%B0%E6%A0%B8%E6%96%B9%E6%B3%95%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="toc-text">3.4.4.1 简述核方法的基本原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-4-2-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%9C%A8%E5%85%A8%E5%9B%BE%E5%B1%82%E9%9D%A2%E7%9A%84%E4%BB%BB%E5%8A%A1%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%A0%B8%E6%96%B9%E6%B3%95%EF%BC%9F"><span class="toc-text">3.4.4.2 为什么在全图层面的任务中使用核方法？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-4-3-%E9%99%A4%E4%BA%86%E4%B8%8A%E8%BF%B0%E6%8F%90%E5%88%B0%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A0%B8%E4%B9%8B%E5%A4%96%EF%BC%8C%E8%BF%98%E6%9C%89%E5%93%AA%E4%BA%9B%E6%A0%B8%EF%BC%9F"><span class="toc-text">3.4.4.3 除了上述提到的两种核之外，还有哪些核？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-5-%E9%82%BB%E6%8E%A5%E7%9F%A9%E9%98%B5"><span class="toc-text">3.4.5 邻接矩阵</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-5-1-%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A0%B8%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B%E4%B8%AD%EF%BC%8C%E5%93%AA%E4%BA%9B%E7%94%A8%E5%88%B0%E4%BA%86%E9%82%BB%E6%8E%A5%E7%9F%A9%E9%98%B5%EF%BC%9F"><span class="toc-text">3.4.5.1 传统机器学习核特征工程中，哪些用到了邻接矩阵？</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-4-5-2-%E5%A6%82%E4%BD%95%E5%B0%86%E6%8A%8A%E6%97%A0%E5%90%91%E5%9B%BE%E7%9A%84%E8%8A%82%E7%82%B9%E3%80%81%E8%BF%9E%E6%8E%A5%E3%80%81%E5%85%A8%E5%9B%BE%E7%9A%84%E7%89%B9%E5%BE%81%E6%8E%A8%E5%B9%BF%E5%88%B0%E6%9C%89%E5%90%91%E5%9B%BE%EF%BC%9F"><span class="toc-text">3.4.5.2 如何将把无向图的节点、连接、全图的特征推广到有向图？</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-6-%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-Weisfeiler-Lehman-%E6%A0%B8%EF%BC%9F"><span class="toc-text">3.4.6 如何使用代码实现 Weisfeiler-Lehman 核？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-%E7%96%91%E9%97%AE"><span class="toc-text">3.5 疑问</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-1-Katz-Index"><span class="toc-text">3.5.1 Katz Index</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-2-Weisfeiler-Kernel"><span class="toc-text">3.5.2 Weisfeiler Kernel</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/03/08/2024/03/2024-03-08-Casual_Inference-03/" title="【基于图模型的因果推断】3 图模型和结构因果模型：理论和简单实践"><img src="/img/header/CasualInference.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【基于图模型的因果推断】3 图模型和结构因果模型：理论和简单实践"/></a><div class="content"><a class="title" href="/2024/03/08/2024/03/2024-03-08-Casual_Inference-03/" title="【基于图模型的因果推断】3 图模型和结构因果模型：理论和简单实践">【基于图模型的因果推断】3 图模型和结构因果模型：理论和简单实践</a><time datetime="2024-03-07T16:00:00.000Z" title="Created 2024-03-08 00:00:00">2024-03-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/05/2024/03/2024-03-05-Casual_Inference-01/" title="【基于图模型的因果推断】1 绪论"><img src="/img/header/CasualInference.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【基于图模型的因果推断】1 绪论"/></a><div class="content"><a class="title" href="/2024/03/05/2024/03/2024-03-05-Casual_Inference-01/" title="【基于图模型的因果推断】1 绪论">【基于图模型的因果推断】1 绪论</a><time datetime="2024-03-04T16:00:00.000Z" title="Created 2024-03-05 00:00:00">2024-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/05/2024/03/2024-03-05-Casual_Inference-02/" title="【基于图模型的因果推断】2 数学基础"><img src="/img/header/CasualInference.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【基于图模型的因果推断】2 数学基础"/></a><div class="content"><a class="title" href="/2024/03/05/2024/03/2024-03-05-Casual_Inference-02/" title="【基于图模型的因果推断】2 数学基础">【基于图模型的因果推断】2 数学基础</a><time datetime="2024-03-04T16:00:00.000Z" title="Created 2024-03-05 00:00:00">2024-03-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/01/2024/03/2024-03-01-TypeThTypeScript-02/" title="【类型论前导与TypeScript】2 TypeScript的控制语句"><img src="/img/header/TypeThIntro.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【类型论前导与TypeScript】2 TypeScript的控制语句"/></a><div class="content"><a class="title" href="/2024/03/01/2024/03/2024-03-01-TypeThTypeScript-02/" title="【类型论前导与TypeScript】2 TypeScript的控制语句">【类型论前导与TypeScript】2 TypeScript的控制语句</a><time datetime="2024-02-29T16:00:00.000Z" title="Created 2024-03-01 00:00:00">2024-03-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/01/17/2024/01/2024-01-17-LLM-02/" title="【大模型基础教程】2 大模型能力"><img src="/img/header/LLM.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【大模型基础教程】2 大模型能力"/></a><div class="content"><a class="title" href="/2024/01/17/2024/01/2024-01-17-LLM-02/" title="【大模型基础教程】2 大模型能力">【大模型基础教程】2 大模型能力</a><time datetime="2024-01-16T16:00:00.000Z" title="Created 2024-01-17 00:00:00">2024-01-17</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Ruijie He</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="200" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>