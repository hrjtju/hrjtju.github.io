<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>【Natural Language Processing】1 Word2vec (1) | Random-Walker in the Knowledge Category</title><meta name="author" content="Ruijie He"><meta name="copyright" content="Ruijie He"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Notes for CS224w">
<meta property="og:type" content="article">
<meta property="og:title" content="【Natural Language Processing】1 Word2vec (1)">
<meta property="og:url" content="https://hrjtju.github.io/2024/08/01/2024/08/2024-08-01-NLP-01/index.html">
<meta property="og:site_name" content="Random-Walker in the Knowledge Category">
<meta property="og:description" content="Notes for CS224w">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://hrjtju.github.io/img/header/NLP.png">
<meta property="article:published_time" content="2024-07-31T16:00:00.000Z">
<meta property="article:modified_time" content="2024-08-01T10:58:43.700Z">
<meta property="article:author" content="Ruijie He">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Deep Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hrjtju.github.io/img/header/NLP.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://hrjtju.github.io/2024/08/01/2024/08/2024-08-01-NLP-01/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?84a2e12fa5f41f1d7c86b0eb39ca1eab";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-M1C6DBL0RL"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-M1C6DBL0RL');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '【Natural Language Processing】1 Word2vec (1)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-08-01 18:58:43'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/arisu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">35</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><hr class="custom-hr"/></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/header/NLP.png')"><nav id="nav"><span id="blog-info"><a href="/" title="Random-Walker in the Knowledge Category"><img class="site-icon" src="/img/logo.jpg"/><span class="site-name">Random-Walker in the Knowledge Category</span></a></span><div id="menus"><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">【Natural Language Processing】1 Word2vec (1)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-07-31T16:00:00.000Z" title="Created 2024-08-01 00:00:00">2024-08-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-08-01T10:58:43.700Z" title="Updated 2024-08-01 18:58:43">2024-08-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/posts/">posts</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>12min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="【Natural Language Processing】1 Word2vec (1)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p>Source: CS224n, Natural Language Processing with Deep Learning</p>
</blockquote>
<h1 id="1-Introduction-to-NLP"><a href="#1-Introduction-to-NLP" class="headerlink" title="1 Introduction to NLP"></a>1 Introduction to NLP</h1><ul>
<li>complexity of communication enabled by language is a uniquely human intelligence among species</li>
<li>Human children, interacting with a rich multi-modality world and  various forms of feedback, acquire language with exceptional sample efficiency (not observing that much language) and compute efficiency (brains are efficient computing machines!)</li>
</ul>
<p><strong>A few applications of NLP</strong></p>
<ul>
<li>Machine Translation<ul>
<li>failures of these systems for most of the world’s 7000 languages, difficulties in translating long text, and ensuring contextual correctness of translations make this still a fruitful field of research.</li>
</ul>
</li>
<li>Question answering and information retrieval</li>
<li>Summarization and analysis of text<ul>
<li>NLP tools can be powerful for both the increase of access to information to the public, as well as surveillance, corporate or governmental.</li>
</ul>
</li>
<li>Speech-to-text</li>
</ul>
<blockquote>
<p>In all aspects of NLP, most existing tools work for precious few (usually one, maybe up to 100) of the world’s roughly 7000 languages, and fail disproportionately much on lesser-spoken and/or marginalized dialects, accents, and more. Beyond this, recent successes in building better systems have far outstripped our ability to characterize and audit these systems. Biases encoded in text, from race to gender to religion and more, are reflected and often amplified by NLP systems. With these challenges and considerations in mind, but with the desire to do good science and build trustworthy systems that improve peoples’ lives</p>
</blockquote>
<h1 id="2-Representing-Words"><a href="#2-Representing-Words" class="headerlink" title="2 Representing Words"></a>2 Representing Words</h1><h2 id="2-1-Independent-vectors-for-different-words"><a href="#2-1-Independent-vectors-for-different-words" class="headerlink" title="2.1 Independent vectors for different words"></a>2.1 Independent vectors for different words</h2><ul>
<li>Simplest way to represent words is to use unrelated vectors (as a set)</li>
</ul>
<p><strong>Type</strong> is an element of a finite volcabulary<br><strong>Token</strong> is an instance of a <strong>type</strong> observed in some context</p>
<p>We can store different words as independent standard basis (<strong>1-hot vector</strong>) of $\mathbb{R}^{n}$, i.e.</p>
<script type="math/tex; mode=display">
v_{\text{tea}} = e_{i} = \left[\, \begin{matrix}
0\\0\\1\\\vdots\\0
\end{matrix} \,\right], \quad
v_{\text{coffee}} = e_{j} = \left[\, \begin{matrix}
\vdots\\0\\1\\0\\\vdots
\end{matrix} \,\right] .</script><p>We encode no similarity or relationship in this vectors, for different word vectors have inner product $0$:</p>
<script type="math/tex; mode=display">
\left\langle e_{i}, e_{j} \right\rangle = e_{i}^{\top} e_{j} \equiv 0,\quad i \ne j</script><p>and plus they are apparently not ordered.</p>
<h2 id="2-2-Vectors-from-annotated-discrete-properties"><a href="#2-2-Vectors-from-annotated-discrete-properties" class="headerlink" title="2.2 Vectors from annotated discrete properties"></a>2.2 Vectors from annotated discrete properties</h2><p>We can construct word vector by the relationships that a word posess in the categories of grammar and other words.</p>
<script type="math/tex; mode=display">
v_{\text{tea}} = \left[ \, \begin{matrix}
0 \\ 0 \\ 1 \\ \vdots \\ 1
\end{matrix} \, \right] 
\begin{align}
& \rightarrow \text{plural noun}\\
& \rightarrow \text{3rd singular verb}\\
& \rightarrow \text{hyponym-of-beverage}\\
& \hspace{5em}\vdots\\
& \rightarrow \text{synonym-of-chai}
\end{align}</script><p><strong>Failures of this method</strong></p>
<ol>
<li>Human annotated resources are always lacking in vocabulary compares to methods that can draw a vocabulary from a naturally occuring text source</li>
<li>A tradeoff between dimensionality and utility of the embedding—it takes a very high dimensional vector to represent all of these categories, which is costly and always incomplete.</li>
<li>Human ideas of what the right representations shoud be for text tend to underperform methods that allow data to determine more aspects</li>
</ol>
<h1 id="3-Distributional-semantics-and-Word2vec"><a href="#3-Distributional-semantics-and-Word2vec" class="headerlink" title="3 Distributional semantics and Word2vec"></a>3 Distributional semantics and Word2vec</h1><ul>
<li>We can learn rich representations of items using <strong>deep learning</strong> by means of <strong>unsupervised learning</strong> or (<strong>self-supervised learning</strong>)</li>
<li>Unsupervised learning or self-supervised learning attempts to learning by predicting the remianing parts of data using unmasked parts.</li>
<li><strong>You should know a word by the company it keeps.</strong></li>
<li>Words can be imagined as distributions over the semantic space where <strong>similar words appear nearer</strong>.</li>
</ul>
<h2 id="3-1-Co-occurrance-matrices-and-document-contexts"><a href="#3-1-Co-occurrance-matrices-and-document-contexts" class="headerlink" title="3.1 Co-occurrance matrices and document contexts"></a>3.1 Co-occurrance matrices and document contexts</h2><p>One idea of co-occurrance may be the following</p>
<ol>
<li>Determine a vocabulary $\mathcal{V}$</li>
<li>Construct zero matrix of shape $|\mathcal{V}| \times |\mathcal{V}|$</li>
<li>Count the co-occurrances and fill in the matrix</li>
<li>Normalize rows by their sums</li>
</ol>
<p>Assume the matrix is notated as $\boldsymbol{V}$, then word vectors of each word in vocabulary $\mathcal{V}$ is the rows of the matrix:</p>
<script type="math/tex; mode=display">
\boldsymbol{V} = \left[ \,\begin{matrix}
v_{1}\\v_{2}\\ \vdots \\ v_{|\mathcal{V}|}
\end{matrix}\, \right].</script><p>We can use various criterions to determine whether two words co-occurr, as the following shows:</p>
<script type="math/tex; mode=display">
\left[ \text{It's hot ans delicious.} \left[ \text{I pured} \left[ \text{the } \mathop{ \text{tea} }\limits_{ \blacktriangle } \text{ for} \right]_{\text{1}}\text{my uncle} \right]_{3}  \right]_{\text{document}} ,</script><p>in which the subscripts of the brackets tells the radius of the co-ocurrance range and the black triangle indicates the central word <em>tea</em>.</p>
<p><strong>Failures of this method</strong></p>
<ul>
<li>Higher-dimensional vectors tend to be unwieldy</li>
<li>Raw counts of co-occurrences over-emphasize the importance of common words such as <em>the</em>. [taking log frequency tend to be useful.]</li>
<li>Refer to <a target="_blank" rel="noopener" href="https://www.google.com.hk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwjMrYb_udOHAxUHkO4BHSxaJYkQFnoECBUQAQ&amp;url=https%3A%2F%2Fnlp.stanford.edu%2Fpubs%2Fglove.pdf&amp;usg=AOvVaw3XPTcwWcbYOXnahjvpeDTu&amp;opi=89978449">GloVe (Pennington et al. 2014)</a></li>
</ul>
<h2 id="3-2-Word2vec"><a href="#3-2-Word2vec" class="headerlink" title="3.2 Word2vec"></a>3.2 Word2vec</h2><p><em>Skipgram word2vec</em> model learns a short vector of words by peeking into a small contexts of words. </p>
<p>We have finite vocabulary $\mathcal{V}$, let $C,O \in \mathcal{V}$ be random variables representing unknown pair of words with $C$ representing <em>center word</em> and $O$ representing an <em>outside word</em> and let $c$ and $o$ represent specific values of corresponding values of these random variables.</p>
<p>Let $U, V \in \mathbb{R}^{|\mathcal{V}| \times d}$ be matrices and each word in $\mathcal{V}$ is associated in the corresponding row in $U$ and $V$.</p>
<p><strong>Skipgram word2vec</strong><br>The word2vec model is as follows</p>
<script type="math/tex; mode=display">
p_{U,V}(o|c) = \frac{\exp \left\{ u_{o}^{\top}v_{c} \right\}}{\sum\limits_{w \in \mathcal{V}} \exp \left\{ u_{w}^{\top} v_{c} \right\}  },</script><p>in which $u<em>{w}$ refers to row of $U$ corresponding to word $w \in \mathcal{V}$. Note that $p</em>{U,V}(\cdot|c) \in \mathbb{R}^{|\mathcal{V}|}$<br>is the probabilites of all words given the center word $c$, which is similar to the row in the co-occurrance matrix $\boldsymbol{V}$.</p>
<p>Then we learn to optimize the cross-entropy loss with the true distribution $P^{*}(O|C)$:</p>
<script type="math/tex; mode=display">
\min_{U,V} \mathbb{E}_{O,C} \left[ -\log p_{U,V}(o|c) \right]  = \min_{U,V} \sum_{o,c} -p^{*}(o|c)\log p(o|c).</script><blockquote>
<p><strong>Questions</strong></p>
<ul>
<li>How do we perform the <em>min</em> operation?</li>
<li>How do we get the random variables?</li>
<li>Why the negative-log of the probability?</li>
<li>Why is this much better than the co-occurrance counting?</li>
<li>Why not all distributions over $o$ and $c$ can be represented by this model?</li>
</ul>
</blockquote>
<h2 id="3-3-Estimating-a-word2vec-model-from-a-corpus"><a href="#3-3-Estimating-a-word2vec-model-from-a-corpus" class="headerlink" title="3.3 Estimating a word2vec model from a corpus"></a>3.3 Estimating a word2vec model from a corpus</h2><p><strong>Word2vec empirical loss</strong><br>Consider how we estimate the expectation term in the aforementioned model, we perform empirical loss. Let $D$ be a set of documents ${ d<em>{i} }</em>{i=1}$, where each contains a sequence of word $(w<em>{1}^{(d)}, \dots, w</em>{m}^{(d)})$ with all $w<em>{i}^{(d)} \in \mathcal{V}$ and let $k \in \mathbb{N}</em>{&gt;0}$ be a positive-integer window size. For all center word $w<em>{i}$, we take the words within the window size, i.e. ${ w</em>{i-k}, \dots, w<em>{i-1}, w</em>{i+1}, \dots, w_{i+k} }$ to estimate the expectation term:</p>
<script type="math/tex; mode=display">
\begin{align}
L(U,V) &= \sum\limits_{d \in D} \sum\limits_{i=1}^{m} \sum\limits_{j=1}^{k} -\log p_{U,V} \Big[w_{i-j}^{(d)} \Big| w_{i}^{(d)} \Big]\\
&= \sum\limits_{d \in D} \sum\limits_{i=1}^{m} \sum\limits_{j=1}^{k} -\log \left[ \frac{\exp \left\{ {w_{i-j}^{(d)}}^{\top}w_{i}^{(d)} \right\}}{\sum\limits_{s = 1}^{m} \exp \left\{ {w_{s}^{(d)}}^{\top} w_{i}^{(d)} \right\}} \right] .
\end{align}</script><p><strong>Gradient-based estimation</strong><br>We first initialize the matrices $U$ and $V$ by drawing entries from gaussian distribution such as $\mathcal{N}(0, 1\times e^{-3})$ and preform gradient descent on $U$:</p>
<script type="math/tex; mode=display">
U \leftarrow  U - \alpha \cdot \nabla_{U} L(U,V).</script><p><strong>Stochastic gradient</strong><br>Summing over the entire set of documents can be expansive, so we update matrix $U$ every time when having seen only as few of them:</p>
<script type="math/tex; mode=display">
\hat{L}(U,V) = \sum\limits_{d_{1},\dots,d_{l}} \sum\limits_{i=1}^{m} \sum\limits_{j=1}^{k} -\log p_{U,V} \Big[w_{i-j}^{(d)} \Big| w_{i}^{(d)} \Big]</script><p>for some integer $l \in \mathbb{N}_{&gt;0}$.</p>
<h2 id="3-4-Calculating-gradient"><a href="#3-4-Calculating-gradient" class="headerlink" title="3.4 Calculating gradient"></a>3.4 Calculating gradient</h2><p>By the linearity of gradient operator, we have</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_{v_{c}} \hat{L}(U, V) &= \sum\limits_{d_{1},\dots,d_{l}} \sum\limits_{i=1}^{m} \sum\limits_{j=1}^{k} -\nabla_{v_{c}} \log p_{U,V} \Big[w_{i-j}^{(d)} \Big| w_{i}^{(d)} \Big]\\
&= \sum\limits_{d_{1},\dots,d_{l}} \sum\limits_{i=1}^{m} \sum\limits_{j=1}^{k} -{\color{blue} \nabla_{v_{c}} \left[  \log  \frac{\exp \left\{ u_{o}^{\top}v_{c} \right\}}{\sum\limits_{w \in \mathcal{V}} \exp \left\{ u_{w}^{\top} v_{c} \right\}  } \right] }.
\end{align}</script><p>We can split the term in blue into two terms by the properties of log function and the linearity of gradient operator:</p>
<script type="math/tex; mode=display">
\begin{align}
\nabla_{v_{c}} \left[  \log \frac{\exp \left\{ u_{o}^{\top}v_{c} \right\}}{\sum\limits_{w \in \mathcal{V}} \exp \left\{ u_{w}^{\top} v_{c} \right\}  } \right] 
&= {\color{red} \nabla_{v_{c}} \log \exp \left\{ u_{o}^{\top}v_{c} \right\}} - {\color{blue} \nabla_{v_{c}} \log \sum\limits_{w \in \mathcal{V}} \exp \left\{ u_{w}^{\top} v_{c} \right\}}\\
&= {\color{red} u_{o}} + {\color{blue} \frac{1}{\sum\limits_{w \in \mathcal{V}} \exp \left\{ u_{w}^{\top} v_{c} \right\}}}\nabla_{v_{c}}  \sum\limits_{w' \in \mathcal{V}} \exp \left\{ u_{w'}^{\top} v_{c} \right\} \\
& \hspace{1em}\small\text{(Inverse operation, Chain rule)}\\
&= {\color{red} u_{o}} +  \frac{1}{\sum\limits_{w \in \mathcal{V}} \exp \left\{ u_{w}^{\top} v_{c} \right\}} {\color{blue} \sum\limits_{w' \in \mathcal{V}} \nabla_{v_{c}} } \exp \left\{ u_{w'}^{\top} v_{c} \right\} \\
& \hspace{1em}\small\text{(Lineararity of gradient op.)}\\
&= {\color{red} u_{o}} + \frac{1}{\sum\limits_{w \in \mathcal{V}} \exp \left\{ u_{w}^{\top} v_{c} \right\}}  \sum\limits_{w' \in \mathcal{V}} {\color{blue} \exp \left\{ u_{w'}^{\top} v_{c} \right\} \nabla_{v_{c}} u_{w'}^{\top} v_{c}  }  \\
& \hspace{1em}\small\text{(Chain rule)}\\
&= {\color{red} u_{o}} +  \frac{1}{\sum\limits_{w \in \mathcal{V}} \exp \left\{ u_{w}^{\top} v_{c} \right\}}  \sum\limits_{w' \in \mathcal{V}} \exp \left\{ u_{w'}^{\top} v_{c} \right\} {\color{blue}u_{w'}  }  \\ 
\end{align}</script><p>When observing the second term, one can derive that</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{1}{\sum\limits_{w \in \mathcal{V}} \exp \left\{ u_{w}^{\top} v_{c} \right\}}  \sum\limits_{w' \in \mathcal{V}} \exp \left\{ u_{w'}^{\top} v_{c} \right\} u_{w'} 
&= 
\sum\limits_{w' \in \mathcal{V}} \frac{\exp \left\{ u_{w'}^{\top} v_{c} \right\}}{\sum\limits_{w \in \mathcal{V}} \exp \left\{ u_{w}^{\top} v_{c} \right\}} u_{w'} \\
&= \sum\limits_{w' \in \mathcal{V}} p_{U,V}(w'|c) u_{w'}\\
&= \mathbb{E}[u_{w'}]
\end{align}</script><p>Thus the whole gradient term can be simplified as </p>
<script type="math/tex; mode=display">
\nabla_{v_{c}}\hat{L}(U,V) = \mathop{u_{o}}\limits_{\text{observed}} - \mathop{\mathbb{E}[u_{w}]}\limits_{\text{expected}},</script><p>which is in align with the intuition: upgrade the word vector towards the opposite direction of the expectation.</p>
<h2 id="3-5-Skip-gram-with-negative-sampling-SGNS"><a href="#3-5-Skip-gram-with-negative-sampling-SGNS" class="headerlink" title="3.5 Skip-gram with negative sampling (SGNS)"></a>3.5 Skip-gram with negative sampling (SGNS)</h2><p>Notice that calculating the following softmax function can be expansive due to the denominator:</p>
<script type="math/tex; mode=display">
p_{U,V}(o|c) = \frac{ \exp \left\{ u_{o}^{\top}v_{c} \right\}}{\color{blue} \sum\limits_{w \in \mathcal{V}} \exp \left\{ u_{w}^{\top} v_{c} \right\}  },</script><p>which normalizes the vector. The skip-gram model encourages $u<em>{o}$ and $v</em>{c}$ to be similar and $u<em>{w}$ and $v</em>{c}$ to be different:</p>
<script type="math/tex; mode=display">
\log \sigma(u_{o}^{\top}v_{c}) + \sum\limits_{w=1}^{k} \Big[ \log \sigma (-u_{w}^{\top} v_{c}) \Big],</script><p>where $u<em>{l}$ is drawn from some distribution $p</em>{\text{neg}}$.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://hrjtju.github.io">Ruijie He</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://hrjtju.github.io/2024/08/01/2024/08/2024-08-01-NLP-01/">https://hrjtju.github.io/2024/08/01/2024/08/2024-08-01-NLP-01/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a></div><div class="post_share"><div class="social-share" data-image="/img/header/NLP.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2024/03/15/2024/03/2024-03-15-Casual_Inference-05/" title="【基于图模型的因果推断】5 图模型的Markov等价性和编程实践"><img class="cover" src="/img/header/CasualInference.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">【基于图模型的因果推断】5 图模型的Markov等价性和编程实践</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/01/17/2024/01/2024-01-17-LLM-02/" title="【大模型基础教程】2 大模型能力"><img class="cover" src="/img/header/LLM.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-17</div><div class="title">【大模型基础教程】2 大模型能力</div></div></a></div><div><a href="/2024/01/16/2024/01/2024-01-16-LLM-01/" title="【大模型基础教程】1 大模型基础"><img class="cover" src="/img/header/LLM.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-16</div><div class="title">【大模型基础教程】1 大模型基础</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/arisu.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Ruijie He</div><div class="author-info__description">Undergraduate in Tongji University</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">35</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">2</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/hrjtju"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">7月17日给博客换了一个主题，现在仍在装修中</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Introduction-to-NLP"><span class="toc-text">1 Introduction to NLP</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Representing-Words"><span class="toc-text">2 Representing Words</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Independent-vectors-for-different-words"><span class="toc-text">2.1 Independent vectors for different words</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Vectors-from-annotated-discrete-properties"><span class="toc-text">2.2 Vectors from annotated discrete properties</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Distributional-semantics-and-Word2vec"><span class="toc-text">3 Distributional semantics and Word2vec</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-Co-occurrance-matrices-and-document-contexts"><span class="toc-text">3.1 Co-occurrance matrices and document contexts</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Word2vec"><span class="toc-text">3.2 Word2vec</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-Estimating-a-word2vec-model-from-a-corpus"><span class="toc-text">3.3 Estimating a word2vec model from a corpus</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-Calculating-gradient"><span class="toc-text">3.4 Calculating gradient</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-Skip-gram-with-negative-sampling-SGNS"><span class="toc-text">3.5 Skip-gram with negative sampling (SGNS)</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/08/01/2024/08/2024-08-01-NLP-01/" title="【Natural Language Processing】1 Word2vec (1)"><img src="/img/header/NLP.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【Natural Language Processing】1 Word2vec (1)"/></a><div class="content"><a class="title" href="/2024/08/01/2024/08/2024-08-01-NLP-01/" title="【Natural Language Processing】1 Word2vec (1)">【Natural Language Processing】1 Word2vec (1)</a><time datetime="2024-07-31T16:00:00.000Z" title="Created 2024-08-01 00:00:00">2024-08-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/15/2024/03/2024-03-15-Casual_Inference-05/" title="【基于图模型的因果推断】5 图模型的Markov等价性和编程实践"><img src="/img/header/CasualInference.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【基于图模型的因果推断】5 图模型的Markov等价性和编程实践"/></a><div class="content"><a class="title" href="/2024/03/15/2024/03/2024-03-15-Casual_Inference-05/" title="【基于图模型的因果推断】5 图模型的Markov等价性和编程实践">【基于图模型的因果推断】5 图模型的Markov等价性和编程实践</a><time datetime="2024-03-14T16:00:00.000Z" title="Created 2024-03-15 00:00:00">2024-03-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/09/2024/03/2024-03-09-Casual_Inference-04/" title="【基于图模型的因果推断】4 图模型的分析"><img src="/img/header/CasualInference.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【基于图模型的因果推断】4 图模型的分析"/></a><div class="content"><a class="title" href="/2024/03/09/2024/03/2024-03-09-Casual_Inference-04/" title="【基于图模型的因果推断】4 图模型的分析">【基于图模型的因果推断】4 图模型的分析</a><time datetime="2024-03-08T16:00:00.000Z" title="Created 2024-03-09 00:00:00">2024-03-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/08/2024/03/2024-03-08-Casual_Inference-03/" title="【基于图模型的因果推断】3 图模型和结构因果模型：理论和简单实践"><img src="/img/header/CasualInference.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【基于图模型的因果推断】3 图模型和结构因果模型：理论和简单实践"/></a><div class="content"><a class="title" href="/2024/03/08/2024/03/2024-03-08-Casual_Inference-03/" title="【基于图模型的因果推断】3 图模型和结构因果模型：理论和简单实践">【基于图模型的因果推断】3 图模型和结构因果模型：理论和简单实践</a><time datetime="2024-03-07T16:00:00.000Z" title="Created 2024-03-08 00:00:00">2024-03-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2024/03/05/2024/03/2024-03-05-Casual_Inference-01/" title="【基于图模型的因果推断】1 绪论"><img src="/img/header/CasualInference.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="【基于图模型的因果推断】1 绪论"/></a><div class="content"><a class="title" href="/2024/03/05/2024/03/2024-03-05-Casual_Inference-01/" title="【基于图模型的因果推断】1 绪论">【基于图模型的因果推断】1 绪论</a><time datetime="2024-03-04T16:00:00.000Z" title="Created 2024-03-05 00:00:00">2024-03-05</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Ruijie He</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="200" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>